)
# Choose a performance measure
if (tolower(meas) == "rmse"){
meas = rmse
} else if (tolower(meas) == "mse"){
meas = mse
} else if (tolower(meas) == "sse"){
meas = sse
} else if (tolower(meas) == "mae"){
meas = mae
} else if (tolower(meas) == "medse"){
meas = medse
}
} else if (tolower(model) == "categorical regression") {
lrn <- makeLearner("classif.glmnet",predict.type = "prob")
params <- makeParamSet(
makeNumericParam(id = "alpha",  lower = alphaL, upper = alphaU),
makeIntegerParam(id = "nlambda",  lower = nlambdaL, upper = nlambdaU),
makeNumericParam(id = "lambda", lower = lambdaL, upper = lambdaU),
makeNumericParam(id = "thresh", lower = threshL, upper = threshU),
makeIntegerParam(id = "maxit", lower = maxitL,upper = maxitU)
)
# Choose a performance measure
if (tolower(meas) == "acc"){
meas = acc
} else if (tolower(meas) == "auc"){
meas = auc
} else if (tolower(meas) == "f1"){
meas = f1
} else if (tolower(meas) == "fdr"){
meas = fdr
} else if (tolower(meas) == "fn"){
meas = fn
} else if (tolower(meas) == "fp"){
meas = fp
} else if (tolower(meas) == "fpr"){
meas = fpr
} else if (tolower(meas) == "mmce"){
meas = mmce
} else if (tolower(meas) == "ppv"){
meas = ppv
} else if (tolower(meas) == "tn"){
meas = tn
} else if (tolower(meas) == "tnr"){
meas = tnr
} else if (tolower(meas) == "tp"){
meas = tp
} else if (tolower(meas) == "tpr"){
meas = tpr
}
} #end of logistic
# Choose a tuning method
if (tolower(gridsearch) == "random") {
ctrl = makeTuneControlRandom(maxit = search_maxit)
} else if (tolower(gridsearch) == "complete") {
ctrl = makeTuneControlGrid(resolution = search_reso)
}
# Choose a resampling strategy
if (rdesc_stratify == FALSE) {
if (tolower(rdesc) == "cv") {
rdesc = makeResampleDesc("CV", iters = rdesc_iters)
} else if (tolower(rdesc) == "holdout") {
rdesc = makeResampleDesc("Holdout", split = rdesc_split)
} else if (tolower(rdesc) == "subsample") {
rdesc = makeResampleDesc("Subsample", iters = rdesc_iters, split = rdesc_split)
} else if (tolower(rdesc) == "loo") {
rdesc = makeResampleDesc("LOO")
} else if (tolower(rdesc) == "bootstrap") {
rdesc = makeResampleDesc("Bootstrap", iters = rdesc_iters)
} else if (tolower(rdesc) == "repcv") {
rdesc = makeResampleDesc("RepCV", folds = rdesc_folds , reps = rdesc_reps)
}
} else {
if (tolower(rdesc) == "cv") {
rdesc = makeResampleDesc("CV", iters = rdesc_iters, stratify = TRUE, stratify.cols =  rdesc_stratify_cols)
} else if (tolower(rdesc) == "holdout") {
rdesc = makeResampleDesc("Holdout", split = rdesc_split, stratify = TRUE, stratify.cols =  rdesc_stratify_cols)
} else if (tolower(rdesc) == "subsample") {
rdesc = makeResampleDesc("Subsample", iters = rdesc_iters, split = rdesc_split, stratify = TRUE, stratify.cols =  rdesc_stratify_cols)
} else if (tolower(rdesc) == "loo") {
rdesc = makeResampleDesc("LOO")
} else if (tolower(rdesc) == "bootstrap") {
rdesc = makeResampleDesc("Bootstrap", iters = rdesc_iters, stratify = TRUE, stratify.cols =  rdesc_stratify_cols)
} else if (tolower(rdesc) == "repcv") {
rdesc = makeResampleDesc("RepCV", folds = rdesc_folds , reps = rdesc_reps, stratify = TRUE, stratify.cols =  rdesc_stratify_cols)
}
}
#parameter tuning
mytune <- tuneParams(learner = lrn, task = trainTask,  resampling = rdesc, measures = meas, par.set = params, control = ctrl, show.info = show_info)
#set hyperparameters
lrn_tune <- setHyperPars(lrn,par.vals = mytune$x)
#train model
trained_model <- train(learner = lrn_tune,task = trainTask)
return(list(mytune$x,mytune$y,trained_model))
} #end of model_choice
#### Model should be tuned or not
if (tolower(model) == "continuous regression") {
if (tune == T) {
model_exec = model_choice(model = "continuous regression")
trained_model = model_exec[[3]]
} else {
lrn <- makeLearner("regr.cvglmnet",predict.type = "response")
trained_model = train(lrn, task = trainTask)
}
} else if (tolower(model) == "categorical regression") {
if (tune == T) {
model_exec = model_choice(model = "categorical regression")
trained_model = model_exec[[3]]
} else {
lrn <- makeLearner("classif.cvglmnet",predict.type = "prob")
trained_model = train(lrn, task = trainTask)
}
}
#predict model
test_pred <- predict(trained_model,testTask)
test_pred <- test_pred[[2]]
if (tasktype == "Regression") {
test_pred <- data.frame(ID = df_test[[exclude_vars]],test_pred[,-which(colnames(test_pred) %in% c("id"))])
} else if (tasktype == "Classification"){
test_pred <- data.frame(ID = df_test[[exclude_vars]],test_pred[,-which(colnames(test_pred) %in% c("id","response"))])
}
train_pred <- predict(trained_model,trainTask)
train_pred <- train_pred[[2]]
if (tasktype == "Regression") {
train_pred <- data.frame(ID = df_train[[exclude_vars]],train_pred[,-which(colnames(train_pred) %in% c("id"))])
} else if (tasktype == "Classification"){
train_pred <- data.frame(ID = df_train[[exclude_vars]],train_pred[,-which(colnames(train_pred) %in% c("id","response"))])
}
if (length(levels(traintask[[outcome]])) == 2 | is.numeric(traintask[[outcome]])) {
coef = coef(getLearnerModel(trained_model,more.unwrap = T))
features = coef[,1]
} else {features <- list()
for (i in 1:(length(coef(getLearnerModel(trained_model,more.unwrap = T)))))
{coef = coef(getLearnerModel(trained_model,more.unwrap = T))[i]
features[[i]] = coef[[1]][,1]}
}
if (tune == T) {
return(list(test_pred,train_pred,model_exec[[1]],model_exec[[2]],features))
} else {return(list(test_pred,train_pred,features))}
}
data = readRDS("/stats/projects/all/R_Tools_Development/data/salaries_data.Rds")
res = split(df=data,filetype_in = "dataframe", seed=50, splitkey = "patient_id", trainprop=0.7)
train = res[[1]]
test = res[[2]]
split <- function(df, filetype_in = "dataframe",seed=50, splitkey = NULL, stratifyby = NULL, trainprop=0.70) {
set.seed(seed)
data <- filetype(df=df,filetype_in=filetype_in)
#Check if splitkey has valid column name
if (!is.null(splitkey)) {
if (!(splitkey %in% colnames(data))) {stop("ERROR: splitkey is not a valid column name")}}
#Check if trainprop is valid
if (trainprop > 1 | trainprop <0) {stop("ERROR: trainprop should be <= 1 and >= 0")}
#If unique ID is not required
if (is.null(splitkey)) {
# If stratified sampling is required
if (!is.null(stratifyby)){
list_t <- NULL
list_t <- data[[stratifyby[1]]]
if (length(stratifyby) > 1) {
for (i in 2: length(stratifyby)){
list_t <- list(list_t,data[[stratifyby[i]]])
}
}
sp <- base::split(seq_len(nrow(data)), list_t)
samples <- lapply(sp, function(x) sample(x, replace=F,size=trainprop*length(x)))
train <- data[unlist(samples), ]
test <- data[-unlist(samples),]
} else {
samples <- sample(1:dim(data)[1],replace=F,size=trainprop*dim(data)[1])
train <- data[samples,]
test <- data[-samples,]
}
} else {
unique_id <- unique(data[[splitkey]])
# If stratified sampling is required
if (!is.null(stratifyby)){
list_t <- NULL
list_t <- data[[stratifyby[1]]]
if (length(stratifyby) > 1) {
for (i in 2: length(stratifyby)){
list_t <- list(list_t,data[[stratifyby[i]]])
}
}
sp <- base::split(seq_len(nrow(data)), list_t)
samples <- lapply(sp, function(x) sample(x, replace=F,size=trainprop*length(x)))
samples <- unique_id[unlist(samples)]
train <- data[data[[splitkey]] %in% samples, ]
test <- data[!(data[[splitkey]] %in% samples),]
} else {
samples <- sample(1:dim(data)[1],replace=F,size=trainprop*dim(data)[1])
samples <- unique_id[samples]
train <- data[data[[splitkey]] %in% samples,]
test <- data[!(data[[splitkey]] %in% samples),]
}
}
split.data <- list()
split.data$train <- train
split.data$test <- test
#Check if result is produced correctly
if (dim(train)[1] == 0 | dim(test)[1] == 0) {
stop("ERROR: One or a combination of the following issues have occurred:
1. stratifyby is a numeric variable or has too many levels
2. trainprop in combination with stratifyby is too small to produce a valid data cut")}
return(split.data)
}
res = split(df=data,filetype_in = "dataframe", seed=50, splitkey = "patient_id", trainprop=0.7)
train = res[[1]]
test = res[[2]]
aa<-factor(sample(rep(letters[1:numLlvs], 200), 50)
)
factor(sample(rep(letters[1:4], 200), 50) ->aa
)
factor(sample(rep(letters[1:4], 200), 50)) ->aa
aa
factor(sample(rep(letters[1:numLlvs], 200), 50)) ->bb
factor(sample(rep(letters[1:4], 200), 50)) ->bb
bb
confusionMatrix(aa,bb)
eval_fun(aa, bb, eval_metric = "confusion matrix")
caret::confusionMatrix(aa, bb)
caret::confusionMatrix(bb, aa)
eval_fun <- function(df_pred, df_true, threshold = FALSE, threshold_value = NULL, eval_metric){
require(caret)
require(pROC)
require(Metrics)
#get predicted class based on probability values
if (threshold == TRUE & !is.null(threshold)){
df_pred<-ifelse(df_pred >= threshold_value, 1, 0)
}
#get R2
rsq<-function(df_pred, df_true){
(1- sum((df_pred - df_true) ^ 2)/sum((df_true - mean(df_true))^2))
}
#get ROC PLOT
roc_plot<- function(df_pred, df_true){
pROC_obj <- roc(df_true, df_pred,
smoothed = TRUE,
# arguments for ci
ci=TRUE,
# arguments for plot
plot=TRUE, auc.polygon=TRUE, max.auc.polygon=TRUE, grid=TRUE,
print.auc=TRUE, show.thres=TRUE)
sens.ci <- ci.se(pROC_obj)
plot(sens.ci, type="shape", col="lightblue")
plot(sens.ci, type="bars")
}
df_pred<-as.numeric(df_pred)
df_true<-as.numeric(df_true)
if (eval_metric == "confusion matrix") {
cm<- caret::confusionMatrix(as.factor(df_pred), as.factor(df_true)) #The functions requires that the factors have exactly the same levels
res<-cm
}
else if (eval_metric == "logloss") {
ll<- Metrics::logLoss(df_true, df_pred) #computes the average log loss between two numeric vectors
res<-ll
}
else if (eval_metric == "roc") {
roc<- roc_plot(df_pred, df_true)
res<-roc
}
else if (eval_metric == "rmse") {
rmse<- Metrics::rmse(df_true, df_pred)
res<-rmse
}
else if (eval_metric == "mae") {
mae<- Metrics::mae(df_true, df_pred)
res<-mae
}
else if (eval_metric == "rsqure"){
r2<- rsq(df_pred, df_true)
res<-r2
}
else stop ("ERROR: Please select vaild evaluation matrix")
return(res)
}
eval_fun(aa, bb, eval_metric = "confusion matrix")
eval_fun(xtab$pred, xtab$truth, eval_metric = "confusion matrix")
eval_fun(xtab$pred, xtab$truth, eval_metric = "roc")
590.355 -5.76 - 36
eval_fun <- function(df_pred, df_true, threshold = FALSE, threshold_value = NULL, eval_metric){
require(caret)
require(pROC)
require(Metrics)
#get predicted class based on probability values
if (threshold == TRUE & !is.null(threshold)){
df_pred<-ifelse(df_pred >= threshold_value, 1, 0)
}
#get R2
rsq<-function(df_pred, df_true){
(1- sum((df_pred - df_true) ^ 2)/sum((df_true - mean(df_true))^2))
}
#get ROC PLOT
roc_plot<- function(df_pred, df_true){
pROC_obj <- roc(df_true, df_pred,
smoothed = TRUE,
# arguments for ci
ci=TRUE,
# arguments for plot
plot=TRUE, auc.polygon=TRUE, max.auc.polygon=TRUE, grid=TRUE,
print.auc=TRUE, show.thres=TRUE)
sens.ci <- ci.se(pROC_obj)
plot(sens.ci, type="shape", col="lightblue")
plot(sens.ci, type="bars")
}
df_pred<-as.numeric(df_pred)
df_true<-as.numeric(df_true)
if (eval_metric == "confusion matrix") {
cm<- caret::confusionMatrix(as.factor(df_pred), as.factor(df_true)) #The functions requires that the factors have exactly the same levels
res<-cm
}
else if (eval_metric == "logloss") {
ll<- Metrics::logLoss(df_true, df_pred) #computes the average log loss between two numeric vectors
res<-ll
}
else if (eval_metric == "roc") {
roc<- roc_plot(df_pred, df_true)
res<-roc
}
else if (eval_metric == "rmse") {
rmse<- Metrics::rmse(df_true, df_pred)
res<-rmse
}
else if (eval_metric == "mae") {
mae<- Metrics::mae(df_true, df_pred)
res<-mae
}
else if (eval_metric == "rsqure"){
r2<- rsq(df_pred, df_true)
res<-r2
}
else stop ("ERROR: Please select vaild evaluation matrix")
return(res)
}
warnings()
eval_fun <- function(df_pred, df_true, threshold = FALSE, threshold_value = NULL, eval_metric){
require(caret)
require(pROC)
require(Metrics)
#get predicted class based on probability values
if (threshold == TRUE & !is.null(threshold)){
df_pred<-ifelse(df_pred >= threshold_value, 1, 0)
}
#get R2
rsq<-function(df_pred, df_true){
(1- sum((df_pred - df_true) ^ 2)/sum((df_true - mean(df_true))^2))
}
#get ROC PLOT
roc_plot<- function(df_pred, df_true){
pROC_obj <- roc(df_true, df_pred,
smoothed = TRUE,
# arguments for ci
ci=TRUE,
# arguments for plot
plot=TRUE, auc.polygon=TRUE, max.auc.polygon=TRUE, grid=TRUE,
print.auc=TRUE, show.thres=TRUE)
sens.ci <- ci.se(pROC_obj)
plot(sens.ci, type="shape", col="lightblue")
plot(sens.ci, type="bars")
}
df_pred<-as.numeric(df_pred)
df_true<-as.numeric(df_true)
if (eval_metric == "confusion matrix") {
cm<- caret::confusionMatrix(as.factor(df_pred), as.factor(df_true)) #The functions requires that the factors have exactly the same levels
res<-cm
}
else if (eval_metric == "logloss") {
ll<- Metrics::logLoss(df_true, df_pred) #computes the average log loss between two numeric vectors
res<-ll
}
else if (eval_metric == "roc") {
roc<- roc_plot(df_pred, df_true)
res<-roc
}
else if (eval_metric == "rmse") {
rmse<- Metrics::rmse(df_true, df_pred)
res<-rmse
}
else if (eval_metric == "mae") {
mae<- Metrics::mae(df_true, df_pred)
res<-mae
}
else if (eval_metric == "rsqure"){
r2<- rsq(df_pred, df_true)
res<-r2
}
else stop ("ERROR: Please select vaild evaluation matrix")
return(res)
}
#Test
eval_fun(xtab$pred, xtab$truth, eval_metric = "confusion matrix")
eval_fun(xtab$pred, xtab$truth, eval_metric = "roc")
68*2
warnings()
eval_fun <- function(df_pred, df_true, threshold = FALSE, threshold_value = NULL, eval_metric){
require(caret)
require(pROC)
require(Metrics)
#get predicted class based on probability values
if (threshold == TRUE & !is.null(threshold)){
df_pred<-ifelse(df_pred >= threshold_value, 1, 0)
}
#get R2
rsq<-function(df_pred, df_true){
(1- sum((df_pred - df_true) ^ 2)/sum((df_true - mean(df_true))^2))
}
#get ROC PLOT
roc_plot<- function(df_pred, df_true){
pROC_obj <- roc(df_true, df_pred,
smoothed = TRUE,
# arguments for ci
ci=TRUE,
# arguments for plot
plot=TRUE, auc.polygon=TRUE, max.auc.polygon=TRUE, grid=TRUE,
print.auc=TRUE, show.thres=TRUE)
sens.ci <- ci.se(pROC_obj)
plot(sens.ci, type="shape", col="lightblue")
plot(sens.ci, type="bars")
}
df_pred<-as.numeric(df_pred)
df_true<-as.numeric(df_true)
if (eval_metric == "confusion matrix") {
cm<- caret::confusionMatrix(as.factor(df_pred), as.factor(df_true)) #The functions requires that the factors have exactly the same levels
res<-cm
}
else if (eval_metric == "logloss") {
ll<- Metrics::logLoss(df_true, df_pred) #computes the average log loss between two numeric vectors
res<-ll
}
else if (eval_metric == "roc") {
roc<- roc_plot(df_pred, df_true)
res<-roc
}
else if (eval_metric == "rmse") {
rmse<- Metrics::rmse(df_true, df_pred)
res<-rmse
}
else if (eval_metric == "mae") {
mae<- Metrics::mae(df_true, df_pred)
res<-mae
}
else if (eval_metric == "rsqure"){
r2<- rsq(df_pred, df_true)
res<-r2
}
else stop ("ERROR: Please select vaild evaluation matrix")
return(res)
}
#Test
eval_fun(xtab$pred, xtab$truth, eval_metric = "confusion matrix")
eval_fun(xtab$pred, xtab$truth, eval_metric = "roc")
warnings()
eval_fun <- function(df_pred, df_true, threshold = FALSE, threshold_value = NULL, eval_metric){
require(caret)
require(pROC)
require(Metrics)
#get predicted class based on probability values
if (threshold == TRUE & !is.null(threshold)){
df_pred<-ifelse(df_pred >= threshold_value, 1, 0)
}
#get R2
rsq<-function(df_pred, df_true){
(1- sum((df_pred - df_true) ^ 2)/sum((df_true - mean(df_true))^2))
}
#get ROC PLOT
roc_plot<- function(df_pred, df_true){
pROC_obj <- roc(df_true, df_pred,
smoothed = TRUE,
# arguments for ci
ci=TRUE,
# arguments for plot
plot=TRUE, auc.polygon=TRUE, max.auc.polygon=TRUE, grid=TRUE,
print.auc=TRUE, show.thres=TRUE)
sens.ci <- ci.se(pROC_obj)
plot(sens.ci, type="shape", col="lightblue")
plot(sens.ci, type="bars")
}
df_pred<-as.numeric(df_pred)
df_true<-as.numeric(df_true)
if (eval_metric == "confusion matrix") {
cm<- caret::confusionMatrix(as.factor(df_pred), as.factor(df_true)) #The functions requires that the factors have exactly the same levels
res<-cm
}
else if (eval_metric == "logloss") {
ll<- Metrics::logLoss(df_true, df_pred) #computes the average log loss between two numeric vectors
res<-ll
}
else if (eval_metric == "roc") {
roc<- roc_plot(df_pred, df_true)
res<-roc
}
else if (eval_metric == "rmse") {
rmse<- Metrics::rmse(df_true, df_pred)
res<-rmse
}
else if (eval_metric == "mae") {
mae<- Metrics::mae(df_true, df_pred)
res<-mae
}
else if (eval_metric == "rsqure"){
r2<- rsq(df_pred, df_true)
res<-r2
}
else stop ("ERROR: Please select vaild evaluation matrix")
return(res)
}
#Test
eval_fun(xtab$pred, xtab$truth, eval_metric = "confusion matrix")
eval_fun(xtab$pred, xtab$truth, eval_metric = "roc")
1300*6.7
1200*6.7
2311*6.7
1980*6.7
